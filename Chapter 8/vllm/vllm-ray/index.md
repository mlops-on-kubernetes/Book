# Deploying vLLM on Ray

This guide walks through the steps to deploy [vLLM](https://github.com/vllm-project/vllm) on a Ray cluster running in Kubernetes.&#x20;

## Step 1: Prepare Karpenter Settings

Before deploying vLLM, make two essential adjustments to ensure model downloads and container images are handled properly during initialization.

### 1.1 Extend Karpenter Node TTL

If you're using a general-purpose Karpenter node pool, update its configuration to ensure it does not terminate nodes too quickly. The default TTL may cause instance termination in as little as 30 seconds if there are no ready pods. Since the Ray container image is approximately 11 GB, allow more time for image pull and initialization.

Set `ttlSecondsAfterEmpty` to a higher value, such as 600 seconds (10 minutes), in your provisioner:

```yaml
spec:
  ttlSecondsAfterEmpty: 600
```

### 1.2 Increase Ephemeral Storage for Model Caching

The default ephemeral storage might not be sufficient for models like Mistral 7B. Increase the node's ephemeral storage to at least 200Gi by modifying the corresponding `EC2NodeClass`:

```yaml
spec:
  ephemeralStorage:
    size: 200Gi
```

This ensures enough temporary space is available on the node to download and cache the model files.

---

## Step 2: Configure Hugging Face API Access

To download models from the Hugging Face Hub, vLLM needs access to your Hugging Face account. This is typically done using a Hugging Face API token.

### 2.1 Generate a Hugging Face API Token

1. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
2. Create a new token with `read` access
3. Copy the token securely — you will use it in the next step

### 2.2 Create a Kubernetes Secret for the Token

Use the following command to create a secret in your Kubernetes cluster:

```bash
kubectl create secret generic hf-token \
  --from-literal=HUGGING_FACE_HUB_TOKEN=<your-token-here>
```

Replace `<your-token-here>` with the actual token you copied in step 2.1.

This secret will be mounted into the vLLM pods so the engine can authenticate with Hugging Face to pull models.

---

## Step 3: Deploy vLLM with RayService

Once the secret is created, you can deploy the vLLM workload using the provided RayService manifest \`ray-service.vllm.yaml\`. 

Apply the manifest using the following command:

> **Note:** The `rayproject/ray-ml:2.33.0.914af0-py311` Docker image is approximately 11 GB in size. On an `m5a.xlarge` EC2 instance, which supports up to 10 Gbps network throughput, pulling this image can take several minutes depending on current network conditions. Be patient—this is expected behavior.

```bash
kubectl apply -f ray-service.vllm.yaml
```

This will deploy a Ray cluster with vLLM configured to serve the [Mistral 7B Instruct model](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2). The service exposes an HTTP route for inference requests.

---

## Step 4: Send Inference Requests and Monitor Status

To monitor deployment status, you can open the Ray Dashboard at any time.

### 4.0 Open the Ray Dashboard

1. Set up port forwarding for the dashboard (replace with your head service name):

```bash
kubectl port-forward svc/mistral-7b-raycluster-<suffix>-head-svc 8265:8265
```

2. In your browser, go to [http://localhost:8265](http://localhost:8265)
3. Click the **Serve** tab to see the status of your deployed model

### 4.1 Get the Head Service Name

The head service name is autogenerated and may not be predictable. To retrieve it, run:

```bash
kubectl get svc | grep head-svc
```

Look for a service name similar to `mistral-7b-raycluster-xxxxx-head-svc`.

### 4.2 Port Forward the Head Service

Use the service name from the previous step to port forward:

```bash
kubectl port-forward svc/<head-service-name> 8000:8000
```

Replace `<head-service-name>` with the actual name you retrieved.

### 4.3 Send a Test Request

Use `curl` to send a test prompt:

```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistralai/Mistral-7B-Instruct-v0.2",
    "messages": [
      {"role": "user", "content": "What is Ray Serve?"}
    ]
  }'
```

You should receive a streaming response or a JSON payload with the model's completion.

If you see an error like this:

```
Path '/v1/chat/completions' not found. Ping http://localhost:8000/-/routes for available routes.
```

It means the model server has not yet fully initialized. Wait a minute and try again once the `/v1/chat/completions` route appears in the output of:

```bash
curl http://localhost:8000/-/routes
```

---

## Step 5: Clean Up Resources

When you're done testing, clean up the deployed resources to avoid unnecessary costs:

### 5.1 Delete the RayService

```bash
kubectl delete -f ray-service.vllm.yaml
```

### 5.2 Delete the Hugging Face Secret

```bash
kubectl delete secret hf-token
```

### 5.3 (Optional) Revert Karpenter Changes

If you made temporary changes to your Karpenter node pools or NodeClass (e.g., TTL or ephemeral storage), consider reverting them to their original values.

